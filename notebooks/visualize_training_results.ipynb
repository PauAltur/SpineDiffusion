{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Workspace setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import pandas as pd\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "from spinediffusion.models.diffusion_models import UnconditionalDiffusionModel\n",
    "from pathlib import Path\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. CSV logs\n",
    "\n",
    "This data has been previously transformed to csv format at the end of training by a pytorch callback and saved to disk. For more information refer to the source code of the `GenerateCSVLog` within the `callbacks.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_paths = [\n",
    "    Path(\n",
    "        f\"P:\\\\Projects\\\\LMB_4Dspine\\\\Iship_Pau_Altur_Pastor\\\\4_training_logs\\\\logs\\\\depthmap\\\\version_{i}\\events.csv\"\n",
    "    )\n",
    "    for i in range(6, 11)\n",
    "]\n",
    "\n",
    "df_tf = pd.DataFrame(columns=[\"run_name\", \"time\", \"tag\", \"value\"])\n",
    "\n",
    "for path in log_paths:\n",
    "    run_name = path.parent.stem\n",
    "\n",
    "    df_run = pd.read_csv(path)\n",
    "    df_run[\"run_name\"] = run_name\n",
    "\n",
    "    df_tf = pd.concat([df_tf, df_run])\n",
    "\n",
    "df_tf = df_tf.sort_values(by=[\"run_name\", \"tag\", \"time\"])\n",
    "df_tf[\"step\"] = df_tf.groupby([\"run_name\", \"tag\"]).cumcount()\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf.sort_values([\"run_name\", \"tag\", \"time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Load config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "\n",
    "for path in log_paths:\n",
    "    run_name = path.parent.stem\n",
    "\n",
    "    with open(path.parent / \"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    configs[run_name] = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in configs:\n",
    "    df_tf.loc[df_tf[\"run_name\"] == run, \"lr\"] = configs[run][\"optimizer\"][\"init_args\"][\n",
    "        \"lr\"\n",
    "    ]\n",
    "\n",
    "    max_epochs = configs[run][\"trainer\"][\"max_epochs\"]\n",
    "    max_steps = df_tf.loc[df_tf[\"run_name\"] == run, \"step\"].max()\n",
    "    df_tf.loc[df_tf[\"run_name\"] == run, \"epoch\"] = (\n",
    "        df_tf.loc[df_tf[\"run_name\"] == run, \"step\"] // max_steps\n",
    "    ) * max_epochs\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\n",
    "    \"train_loss_step\",\n",
    "    \"MSELoss\",\n",
    "    \"PSNR\",\n",
    "    \"SSIM\",\n",
    "    \"val_loss_step\",\n",
    "    \"val_loss_epoch\",\n",
    "    \"train_loss_epoch\",\n",
    "]\n",
    "\n",
    "for run in df_tf.run_name.unique():\n",
    "    df_run = df_tf[df_tf.run_name == run]\n",
    "    lr = df_run.lr.unique()[0]\n",
    "\n",
    "    for key in keys:\n",
    "        df_run_key = df_run[df_run.tag == key]\n",
    "        plt.plot(df_run_key.value)\n",
    "        plt.title(f\"{run} - {lr}\")\n",
    "        plt.xlabel(\"time\")\n",
    "        plt.ylabel(key)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_tf, x=\"step\", y=\"value\", hue=\"lr\", col=\"tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    for run in df_tf.run_name.unique():\n",
    "        df_run_key = df_tf[(df_tf.run_name == run) & (df_tf.tag == key)]\n",
    "        plt.plot(df_run_key.value, label=configs[run][\"optimizer\"][\"init_args\"][\"lr\"])\n",
    "\n",
    "    plt.title(f\"{key} vs step\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(key)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare train and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in df_tf.run_name.unique():\n",
    "    df_run = df_tf[df_tf.run_name == run]\n",
    "\n",
    "    plt.plot(\n",
    "        df_run[df_run.tag == \"train_loss_step\"].step,\n",
    "        df_run[df_run.tag == \"train_loss_step\"].value,\n",
    "        label=\"train_loss\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        df_run[df_run.tag == \"val_loss_step\"].step,\n",
    "        df_run[df_run.tag == \"val_loss_step\"].value,\n",
    "        label=\"val_loss\",\n",
    "    )\n",
    "    plt.title(f\"{run} - lr : {df_run.lr.unique()[0]}\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\"talk\")\n",
    "colors = [\"r\", \"g\", \"b\"]\n",
    "runs = [\"version_7\", \"version_8\", \"version_9\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "for run, color in zip(runs, colors):\n",
    "    df_run = df_tf[df_tf.run_name == run]\n",
    "    lr = df_run.lr.unique()[0]\n",
    "\n",
    "    ax.plot(\n",
    "        df_run[df_run.tag == \"train_loss_step\"].step,\n",
    "        df_run[df_run.tag == \"train_loss_step\"].value,\n",
    "        color + \"-\",\n",
    "        label=f\"{run} - lr : {lr}\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        df_run[df_run.tag == \"val_loss_step\"].step,\n",
    "        df_run[df_run.tag == \"val_loss_step\"].value,\n",
    "        color + \"--\",\n",
    "        label=f\"{run} - lr : {lr}\",\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"step\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"version_7\"\n",
    "\n",
    "config = configs[run_name]\n",
    "ckpt_path = glob.glob(\n",
    "    str(\n",
    "        [path for path in log_paths if run_name in str(path)][0].parent\n",
    "        / \"checkpoints\"\n",
    "        / \"*.ckpt\"\n",
    "    )\n",
    ")[0]\n",
    "\n",
    "model = UNet2DModel(**config[\"model\"][\"init_args\"][\"model\"][\"init_args\"])\n",
    "scheduler = DDPMScheduler(**config[\"model\"][\"init_args\"][\"scheduler\"][\"init_args\"])\n",
    "loss = eval(config[\"model\"][\"init_args\"][\"loss\"][\"class_path\"])(\n",
    "    **config[\"model\"][\"init_args\"][\"loss\"][\"init_args\"]\n",
    ")\n",
    "metrics = []\n",
    "for metric_dict in config[\"model\"][\"init_args\"][\"metrics\"].values():\n",
    "    metric = eval(metric_dict[\"class_path\"])(**metric_dict[\"init_args\"])\n",
    "    metrics.append(metric)\n",
    "\n",
    "\n",
    "lightning_model = UnconditionalDiffusionModel.load_from_checkpoint(\n",
    "    ckpt_path, model=model, scheduler=scheduler, loss=loss, metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "n_channels = 1\n",
    "height = config[\"data\"][\"init_args\"][\"transform_args\"][\"project_to_plane\"][\"height\"]\n",
    "width = config[\"data\"][\"init_args\"][\"transform_args\"][\"project_to_plane\"][\"width\"]\n",
    "\n",
    "input_noise = torch.randn(1, batch_size, n_channels, height, width)\n",
    "generated_images = lightning_model.predict_step(input_noise, 38474)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "n_rows = np.ceil(generated_images.shape[0] / n_cols).astype(int)\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 20))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    if i < generated_images.shape[0]:\n",
    "        ax.imshow(generated_images[i, 0].cpu().numpy(), cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
