{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Workspace setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are activating two extensions:\n",
    "- [Autoreload](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) to automatically reload modules when they change. Very useful when you are working on code in python files and want to test it in the notebook.\n",
    "- [Jupyter-black](https://github.com/drillan/jupyter-black) to format the code cells with the black formatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we import all of the modules needed to visualize the results of the trained diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import pandas as pd\n",
    "import diffusers\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "from spinediffusion.models.diffusion_models import UnconditionalDiffusionModel\n",
    "from spinediffusion.datamodule.datamodule import SpineDataModule\n",
    "from spinediffusion.utils.misc import find_test_param\n",
    "from pathlib import Path\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define analysis\n",
    "\n",
    "The cells below controls the analysis that will be performed in this notebook. We will define the paths from which the model checkpoints and event logs will be loaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Log paths\n",
    "\n",
    "The paths to the logs and checkpoints of the models are defined below. They are defined through three variables:\n",
    "- `versions`: The version number of each training run.\n",
    "- `logdir`: The path to the parent directory where the logs are stored.\n",
    "- `logsubdir`: The subdirectory where the versions are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version numbers to load\n",
    "versions = [20]\n",
    "logdir = Path(\"P:/Projects/LMB_4Dspine/Iship_Pau_Altur_Pastor/4_training_logs/logs\")\n",
    "logsubdir = Path(\"depthmap\")\n",
    "\n",
    "# compose the log paths\n",
    "log_paths = [logdir / logsubdir / f\"version_{i}\" for i in versions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Auxiliary variables\n",
    "\n",
    "In this cell you can define a number of variables that control which analyses will be performed:\n",
    "- `test_param`: Defines whether a sweep for a specific parameter has been performed and thus whether a comparison of the metrics for each parameter value should be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_param = None\n",
    "axis_scale = \"log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. CSV logs\n",
    "\n",
    "This data has been previously transformed to csv format at the end of training by a pytorch callback and saved to disk. For more information refer to the source code of the `GenerateCSVLog` within the `callbacks.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = pd.DataFrame(columns=[\"run_name\", \"time\", \"tag\", \"value\"])\n",
    "\n",
    "for path in log_paths:\n",
    "    run_name = path.stem\n",
    "\n",
    "    df_run = pd.read_csv(path / \"events.csv\")\n",
    "    df_run[\"run_name\"] = run_name\n",
    "\n",
    "    df_tf = pd.concat([df_tf, df_run])\n",
    "\n",
    "df_tf = df_tf.sort_values(by=[\"run_name\", \"tag\", \"time\"])\n",
    "df_tf[\"step\"] = df_tf.groupby([\"run_name\", \"tag\"]).cumcount()\n",
    "\n",
    "df_tf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Load config files\n",
    "\n",
    "The configuration files are loaded to extract the parameters used in the training of the models. They contain all of the information needed to reproduce the training of the models and to know what exactly where the parameters used in that specific run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "\n",
    "for path in log_paths:\n",
    "    run_name = path.stem\n",
    "    with open(path / \"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    configs[run_name] = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Combine them\n",
    "\n",
    "Specific data from the config files is combined with the csv logs to create a single dataframe that contains all of the information needed to analyze the training of the models. These include things like an epoch number and the `test_param` value.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in configs:\n",
    "    df_tf.loc[df_tf[\"run_name\"] == run, test_param] = find_test_param(\n",
    "        configs[run], test_param\n",
    "    )\n",
    "\n",
    "    max_epochs = configs[run][\"trainer\"][\"max_epochs\"]\n",
    "    max_steps = df_tf.loc[df_tf[\"run_name\"] == run, \"step\"].max()\n",
    "    df_tf.loc[df_tf[\"run_name\"] == run, \"epoch\"] = (\n",
    "        df_tf.loc[df_tf[\"run_name\"] == run, \"step\"] * max_epochs\n",
    "    ) // max_steps\n",
    "    df_tf.loc[df_tf[\"run_name\"] == run, \"epoch_fraction\"] = (\n",
    "        df_tf.loc[df_tf[\"run_name\"] == run, \"step\"] * max_epochs\n",
    "    ) / max_steps\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\n",
    "    \"train_loss_step\",\n",
    "    \"MSELoss_step\",\n",
    "    \"PSNR_step\",\n",
    "    \"SSIM_step\",\n",
    "    \"val_loss_step\",\n",
    "    \"val_loss_epoch\",\n",
    "    \"train_loss_epoch\",\n",
    "]\n",
    "\n",
    "for run in df_tf.run_name.unique():\n",
    "    df_run = df_tf[df_tf.run_name == run]\n",
    "    param_val = df_run[test_param].unique()[0]\n",
    "\n",
    "    for key in keys:\n",
    "        df_run_key = df_run[df_run.tag == key]\n",
    "        plt.plot(df_run_key.epoch_fraction, df_run_key.value)\n",
    "        plt.title(f\"{run} - {test_param} : {param_val}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        pretty_key = key.replace(\"_step\", \"\").replace(\"_epoch\", \"\").replace(\"_\", \" \")\n",
    "        plt.ylabel(pretty_key)\n",
    "        if axis_scale == \"log\":\n",
    "            plt.yscale(\"log\")\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Compare runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    for run in df_tf.run_name.unique():\n",
    "        df_run_key = df_tf[(df_tf.run_name == run) & (df_tf.tag == key)]\n",
    "        plt.plot(\n",
    "            df_run_key.epoch_fraction,\n",
    "            df_run_key.value,\n",
    "            label=find_test_param(configs[run], test_param),\n",
    "        )\n",
    "\n",
    "    pretty_key = key.replace(\"_step\", \"\").replace(\"_epoch\", \"\").replace(\"_\", \" \")\n",
    "    plt.title(f\"{pretty_key} vs epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(pretty_key)\n",
    "    if axis_scale == \"log\":\n",
    "        plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Compare train and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in df_tf.run_name.unique():\n",
    "    df_run = df_tf[df_tf.run_name == run]\n",
    "\n",
    "    plt.plot(\n",
    "        df_run[df_run.tag == \"train_loss_epoch\"].epoch_fraction,\n",
    "        df_run[df_run.tag == \"train_loss_epoch\"].value,\n",
    "        label=\"train loss\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        df_run[df_run.tag == \"val_loss_epoch\"].epoch_fraction,\n",
    "        df_run[df_run.tag == \"val_loss_epoch\"].value,\n",
    "        label=\"val loss\",\n",
    "    )\n",
    "    plt.title(f\"{run} - {test_param} : {df_run[test_param].unique()[0]}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    if axis_scale == \"log\":\n",
    "        plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load models from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_models = {}\n",
    "\n",
    "for run in df_tf.run_name.unique():\n",
    "    df_run = df_tf[df_tf.run_name == run]\n",
    "    best_val_loss = df_run.loc[df_run.tag == \"val_loss_epoch\", \"value\"].min()\n",
    "    print(f\"{run} - best val loss: {best_val_loss}\")\n",
    "\n",
    "    ckpt_path = glob.glob(str(logdir / logsubdir / run / \"checkpoints\" / \"*.ckpt\"))[0]\n",
    "\n",
    "    config = configs[run]\n",
    "    model = UNet2DModel(**config[\"model\"][\"init_args\"][\"model\"][\"init_args\"])\n",
    "    if isinstance(config[\"model\"][\"init_args\"][\"scheduler\"], dict):\n",
    "        scheduler = eval(config[\"model\"][\"init_args\"][\"scheduler\"][\"class_path\"])(\n",
    "            **config[\"model\"][\"init_args\"][\"scheduler\"][\"init_args\"]\n",
    "        )\n",
    "    else:\n",
    "        scheduler = eval(config[\"model\"][\"init_args\"][\"scheduler\"])()\n",
    "    loss = eval(config[\"model\"][\"init_args\"][\"loss\"][\"class_path\"])(\n",
    "        **config[\"model\"][\"init_args\"][\"loss\"][\"init_args\"]\n",
    "    )\n",
    "    metrics = []\n",
    "    for metric_dict in config[\"model\"][\"init_args\"][\"metrics\"].values():\n",
    "        metric = eval(metric_dict[\"class_path\"])(**metric_dict[\"init_args\"])\n",
    "        metrics.append(metric)\n",
    "\n",
    "    lightning_model = UnconditionalDiffusionModel.load_from_checkpoint(\n",
    "        ckpt_path, model=model, scheduler=scheduler, loss=loss, metrics=metrics\n",
    "    )\n",
    "    lightning_models[run] = lightning_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../configs/config_uncond.yaml\", \"r\") as f:\n",
    "    data_config = yaml.safe_load(f)\n",
    "\n",
    "data_config[\"data\"][\"init_args\"][\"predict_size\"] = 16\n",
    "\n",
    "datamodule = SpineDataModule(**data_config[\"data\"][\"init_args\"])\n",
    "datamodule.setup(\"test\")\n",
    "\n",
    "predict_dataloader = datamodule.predict_dataloader()\n",
    "\n",
    "trainer = Trainer()\n",
    "generated_images = {}\n",
    "\n",
    "for run, lightning_model in lightning_models.items():\n",
    "    generated_images[run] = trainer.predict(\n",
    "        lightning_model, dataloaders=predict_dataloader\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run, images in generated_images.items():\n",
    "\n",
    "    n_cols = 4\n",
    "    n_rows = np.ceil(images.shape[0] / n_cols).astype(int)\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(20, 20))\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        if i < images.shape[0]:\n",
    "            img = images[i, 0].cpu().numpy()\n",
    "            img[img < 0] = 0\n",
    "            ax.imshow(img, cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Compute Frechet Inception Distance (FID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "real_images = datamodule.train_data[:16][0]\n",
    "\n",
    "real_images = torch.cat([real_images] * 3, dim=1)\n",
    "\n",
    "for run, images in generated_images.items():\n",
    "    images = torch.cat([images] * 3, dim=1)\n",
    "    images[images < 0] = 0\n",
    "\n",
    "    # transform images to 255 range with dtype uint8\n",
    "    real_images = (real_images * 255).byte()\n",
    "    images = (images * 255).byte()\n",
    "\n",
    "    fid = FrechetInceptionDistance()\n",
    "\n",
    "    fid.update(real_images, real=True)\n",
    "    fid.update(images, real=False)\n",
    "\n",
    "    print(f\"{run} - FID : {fid.compute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
