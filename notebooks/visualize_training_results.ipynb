{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Workspace setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "import pandas as pd\n",
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "from spinediffusion.models.diffusion_models import UnconditionalDiffusionModel\n",
    "from pathlib import Path\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. CSV logs\n",
    "\n",
    "This data has been previously transformed to csv format at the end of training by a pytorch callback and saved to disk. For more information refer to the source code of the `GenerateCSVLog` within the `callbacks.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_paths = [\n",
    "    Path(\n",
    "        f\"P:\\\\Projects\\\\LMB_4Dspine\\\\Iship_Pau_Altur_Pastor\\\\4_training_logs\\\\logs\\\\depthmap\\\\version_{i}\\events.csv\"\n",
    "    )\n",
    "    for i in range(6, 11)\n",
    "]\n",
    "\n",
    "df_tf = pd.DataFrame(columns=[\"run_name\", \"time\", \"tag\", \"value\"])\n",
    "\n",
    "for path in log_paths:\n",
    "    run_name = path.parent.stem\n",
    "\n",
    "    df_run = pd.read_csv(path)\n",
    "    df_run[\"run_name\"] = run_name\n",
    "\n",
    "    df_tf = pd.concat([df_tf, df_run])\n",
    "\n",
    "df_tf = df_tf.sort_values(by=[\"run_name\", \"tag\", \"time\"])\n",
    "df_tf[\"step\"] = df_tf.groupby([\"run_name\", \"tag\"]).cumcount()\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf.sort_values([\"run_name\", \"tag\", \"time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Load config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {}\n",
    "\n",
    "for path in log_paths:\n",
    "    run_name = path.parent.stem\n",
    "\n",
    "    with open(path.parent / \"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    configs[run_name] = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in configs:\n",
    "    df_tf[df_tf[\"run_name\"] == run][\"lr\"] = configs[run][\"optimizer\"][\"init_args\"][\"lr\"]\n",
    "\n",
    "    max_epochs = configs[run][\"trainer\"][\"max_epochs\"]\n",
    "    max_steps = df_tf[df_tf[\"run_name\"] == run][\"step\"].max()\n",
    "    df_tf[df_tf[\"run_name\"] == run][\"epoch\"] = (\n",
    "        df_tf[df_tf[\"run_name\"] == run][\"step\"] // max_steps\n",
    "    ) * max_epochs\n",
    "\n",
    "df_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot training curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\n",
    "    \"train_loss_step\",\n",
    "    \"MSELoss\",\n",
    "    \"PSNR\",\n",
    "    \"SSIM\",\n",
    "    \"val_loss_step\",\n",
    "    \"val_loss_epoch\",\n",
    "    \"train_loss_epoch\",\n",
    "]\n",
    "\n",
    "for run in df_tf.run_name.unique():\n",
    "    df_run = df_tf[df_tf.run_name == run]\n",
    "    lr = configs[run][\"optimizer\"][\"init_args\"][\"lr\"]\n",
    "\n",
    "    for key in keys:\n",
    "        df_run_key = df_run[df_run.tag == key]\n",
    "        plt.plot(df_run_key.value)\n",
    "        plt.title(f\"{run} - {lr}\")\n",
    "        plt.xlabel(\"time\")\n",
    "        plt.ylabel(key)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keys:\n",
    "    for run in df_tf.run_name.unique():\n",
    "        df_run_key = df_tf[(df_tf.run_name == run) & (df_tf.tag == key)]\n",
    "        plt.plot(df_run_key.value, label=configs[run][\"optimizer\"][\"init_args\"][\"lr\"])\n",
    "\n",
    "    plt.title(f\"{key} vs step\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(key)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize tensorflow logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf = pd.DataFrame()\n",
    "\n",
    "for e in summary_iterator(\n",
    "    str(log_path / \"events.out.tfevents.1719496170.Portatil_Pau.95140.0\")\n",
    "):\n",
    "    if len(e.summary.value) == 0:\n",
    "        continue\n",
    "    index = e.wall_time\n",
    "    tag = e.summary.value[0].tag\n",
    "    value = e.summary.value[0].simple_value\n",
    "    df_tf.loc[index, tag] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.diff(df_tf.index.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
