{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Workspace setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import open3d as o3d\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"paper\")\n",
    "\n",
    "# set rcParams fontsize to 12 and fontfamily to serif\n",
    "plt.rcParams.update({\"font.size\": 12, \"font.family\": \"serif\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Count number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"P:\\\\Projects\\\\LMB_4Dspine\\\\back_scan_database\")\n",
    "\n",
    "backscan_dirs = glob.glob(str(path / \"**\" / \"*.ply\"))\n",
    "metadata_dirs = glob.glob(str(path / \"**\" / \"*.json\"))\n",
    "\n",
    "dirs_df = pd.DataFrame({\"backscan\": backscan_dirs, \"metadata\": metadata_dirs})\n",
    "dirs_df[\"raw\"] = dirs_df[\"backscan\"].str.contains(\"raw\")\n",
    "dirs_df[\"processed\"] = dirs_df[\"backscan\"].str.contains(\"processed\")\n",
    "\n",
    "dirs_df[\"balgrist\"] = dirs_df[\"backscan\"].str.contains(\"balgrist\")\n",
    "dirs_df[\"croatian\"] = dirs_df[\"backscan\"].str.contains(\"croatian\")\n",
    "dirs_df[\"italian\"] = dirs_df[\"backscan\"].str.contains(\"italian\")\n",
    "dirs_df[\"ukbb\"] = dirs_df[\"backscan\"].str.contains(\"ukbb\")\n",
    "\n",
    "dirs_df.groupby(\"raw\")[[\"balgrist\", \"croatian\", \"italian\", \"ukbb\"]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load one subject each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal_id = \"balgrist_1\"\n",
    "cro_id = \"croatian_1\"\n",
    "it_id = \"italian_10\"\n",
    "ukbb_id = \"ukbb_1238\"\n",
    "\n",
    "backscan_bal = o3d.io.read_point_cloud(str(path / bal_id / f\"{bal_id}_processed.ply\"))\n",
    "backscan_cro = o3d.io.read_point_cloud(str(path / cro_id / f\"{cro_id}_processed.ply\"))\n",
    "backscan_it = o3d.io.read_point_cloud(str(path / it_id / f\"{it_id}_processed.ply\"))\n",
    "backscan_ukbb = o3d.io.read_point_cloud(\n",
    "    str(path / ukbb_id / f\"{ukbb_id}_processed.ply\")\n",
    ")\n",
    "\n",
    "with open(path / bal_id / f\"{bal_id}_metadata_processed.json\", \"r\") as f:\n",
    "    metadata_bal = json.load(f)\n",
    "\n",
    "with open(path / cro_id / f\"{cro_id}_metadata_processed.json\", \"r\") as f:\n",
    "    metadata_cro = json.load(f)\n",
    "\n",
    "with open(path / it_id / f\"{it_id}_metadata_processed.json\", \"r\") as f:\n",
    "    metadata_it = json.load(f)\n",
    "\n",
    "with open(path / ukbb_id / f\"{ukbb_id}_metadata_processed.json\", \"r\") as f:\n",
    "    metadata_ukbb = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_keys = {}\n",
    "\n",
    "metadata_keys[\"balgrist\"] = metadata_bal.keys()\n",
    "metadata_keys[\"croatian\"] = metadata_cro.keys()\n",
    "metadata_keys[\"italian\"] = metadata_it.keys()\n",
    "metadata_keys[\"ukbb\"] = metadata_ukbb.keys()\n",
    "\n",
    "metadata_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common keys: id, dataset, age, gender, pcDicomOutput, SpecialPts, esl, isl, status, pipelineSteps\n",
    "\n",
    "Balgrist, italian, ukbb: height, weight, recordingTime, operations\n",
    "\n",
    "Balgrist-only: recordingDatetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata_bal[\"isl\"].keys())\n",
    "print(metadata_cro[\"isl\"].keys())\n",
    "print(metadata_it[\"isl\"].keys())\n",
    "print(metadata_ukbb[\"isl\"].keys())\n",
    "\n",
    "print(metadata_bal[\"esl\"].keys())\n",
    "print(metadata_cro[\"esl\"].keys())\n",
    "print(metadata_it[\"esl\"].keys())\n",
    "print(metadata_ukbb[\"esl\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Balgrist and UKBB, the esl and isl dictionary keys are 'pcdicomApp'. For croatian the esl and isl dictionary keys are 'formetric'. The italian dataset has both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balgrist ESL: \", np.asarray(metadata_bal[\"isl\"][\"pcdicomapp\"]).shape)\n",
    "print(\"Croatian ESL: \", np.asarray(metadata_cro[\"isl\"][\"formetric\"]).shape)\n",
    "print(\"Italian pcdicomapp ESL: \", np.asarray(metadata_it[\"isl\"][\"pcdicomapp\"]).shape)\n",
    "print(\"Italian ESL formetric: \", np.asarray(metadata_it[\"isl\"][\"formetric\"]).shape)\n",
    "print(\"UKBB ESL:\", np.asarray(metadata_ukbb[\"isl\"][\"pcdicomapp\"]).shape)\n",
    "\n",
    "print(\"Balgrist ISL: \", np.asarray(metadata_bal[\"esl\"][\"pcdicomapp\"]).shape)\n",
    "print(\"Croatian ISL: \", np.asarray(metadata_cro[\"esl\"][\"formetric\"]).shape)\n",
    "print(\"Italian pcdicomapp ISL: \", np.asarray(metadata_it[\"esl\"][\"pcdicomapp\"]).shape)\n",
    "print(\"Italian formetric ISL: \", np.asarray(metadata_it[\"esl\"][\"formetric\"]).shape)\n",
    "print(\"UKBB ISL:\", np.asarray(metadata_ukbb[\"esl\"][\"pcdicomapp\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(metadata_it[\"isl\"][\"pcdicomapp\"])\n",
    "\n",
    "o3d.visualization.draw_geometries([pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore backscans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balgrist backscan: \", backscan_bal)\n",
    "print(\"Croatian backscan: \", backscan_cro)\n",
    "print(\"Italian backscan: \", backscan_it)\n",
    "print(\"UKBB backscan: \", backscan_ukbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dirs = dirs_df[dirs_df[\"processed\"] == True]\n",
    "\n",
    "datasets = [\"croatian\", \"italian\", \"balgrist\", \"ukbb\"]\n",
    "\n",
    "sampled_dirs = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset_dirs = processed_dirs[processed_dirs[\"backscan\"].str.contains(dataset)]\n",
    "    sampled_dirs += list(dataset_dirs.sample(4)[\"backscan\"])\n",
    "\n",
    "# load the backscan data\n",
    "backscans = [o3d.io.read_point_cloud(backscan) for backscan in sampled_dirs]\n",
    "\n",
    "# load the metadata\n",
    "metadatas = [\n",
    "    json.load(\n",
    "        open(backscan.replace(\"ply\", \"json\").replace(\"processed\", \"metadata_processed\"))\n",
    "    )\n",
    "    for backscan in sampled_dirs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the backscans in 3d on a 4x4 grid. Use the same scale for all axes\n",
    "dataset_titles = [\"Nado\", \"IRCCS\", \"UKBB\", \"Balgrist\"]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i, (backscan, metadata) in enumerate(zip(backscans, metadatas)):\n",
    "    ax = fig.add_subplot(4, 4, i + 1, projection=\"3d\")\n",
    "\n",
    "    # color the points according to their z coordinate\n",
    "    c = np.asarray(backscan.points)[:, 2]\n",
    "    ax.scatter(\n",
    "        np.asarray(backscan.points)[:, 0],\n",
    "        np.asarray(backscan.points)[:, 2],\n",
    "        np.asarray(backscan.points)[:, 1],\n",
    "        c=c,\n",
    "        cmap=\"viridis\",\n",
    "        s=0.1,\n",
    "    )\n",
    "\n",
    "    # add ESL and ISL lines to the plot\n",
    "    if dataset_titles[i // 4] == \"Nado\":\n",
    "        sl_key = \"formetric\"\n",
    "    else:\n",
    "        sl_key = \"pcdicomapp\"\n",
    "\n",
    "    ax.scatter(\n",
    "        np.asarray(metadata[\"isl\"][sl_key])[:, 0],\n",
    "        np.asarray(metadata[\"isl\"][sl_key])[:, 2],\n",
    "        np.asarray(metadata[\"isl\"][sl_key])[:, 1],\n",
    "        c=\"r\",\n",
    "        s=0.1,\n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        np.asarray(metadata[\"esl\"][sl_key])[:, 0],\n",
    "        np.asarray(metadata[\"esl\"][sl_key])[:, 2],\n",
    "        np.asarray(metadata[\"esl\"][sl_key])[:, 1],\n",
    "        c=\"b\",\n",
    "        s=0.1,\n",
    "    )\n",
    "\n",
    "    # set lims to max coordinate of the backscan in any axis\n",
    "\n",
    "    lim_max, lim_min = np.max(np.abs(np.asarray(backscan.points))), -np.max(\n",
    "        np.abs(np.asarray(backscan.points))\n",
    "    )\n",
    "    ax.set_xlim(lim_min, lim_max)\n",
    "    ax.set_ylim(lim_min, lim_max)\n",
    "    ax.set_zlim(lim_min, lim_max)\n",
    "\n",
    "    # switch axis direction from negative to positive to positive to negative\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # remove grid, add axis labels in mm, add a title to each row with dataset\n",
    "    ax.set_xlabel(\"X (mm)\")\n",
    "    ax.set_ylabel(\"Z (mm)\")\n",
    "    ax.set_zlabel(\"Y (mm)\")\n",
    "\n",
    "    if i % 4 == 2:\n",
    "        ax.annotate(\n",
    "            dataset_titles[i // 4],\n",
    "            xy=(-0.15, 1.05),\n",
    "            xycoords=\"axes fraction\",\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            fontfamily=\"serif\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spinediffusion.datamodule.transforms.projecting import ProjectToPlane\n",
    "\n",
    "projector = ProjectToPlane(128, 128, 0.9, 1, \"mean\")\n",
    "\n",
    "# project the backscans to the sagittal plane\n",
    "backscans_proj = []\n",
    "for backscan, metadata in zip(backscans, metadatas):\n",
    "    data_id = {}\n",
    "    data_id[\"backscan\"] = backscan\n",
    "    special_points = metadata[\"specialPts\"]\n",
    "    data_id[\"special_points\"] = {k: np.array(v) for k, v in special_points.items()}\n",
    "    backscans_proj.append(projector(data_id)[\"depth_map\"])\n",
    "\n",
    "# plot the projected backscans in 2d on a 4x4 grid\n",
    "\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i, backscan_proj in enumerate(backscans_proj):\n",
    "    ax = fig.add_subplot(4, 4, i + 1)\n",
    "    ax.imshow(backscan_proj.squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    if i % 4 == 2:\n",
    "        ax.annotate(\n",
    "            dataset_titles[i // 4],\n",
    "            xy=(-0.15, 1.05),\n",
    "            xycoords=\"axes fraction\",\n",
    "            ha=\"center\",\n",
    "            fontsize=12,\n",
    "            fontfamily=\"serif\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backscan_proj = backscans_proj[-2]\n",
    "\n",
    "# plot the projected backscan to the left and then a morphologically closed version to the right\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].imshow(backscan_proj.squeeze(), cmap=\"gray\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"Projected backscan\")\n",
    "\n",
    "# do the closing with scipy.ndimage.grey_closing function\n",
    "from scipy.ndimage import grey_closing\n",
    "\n",
    "backscan_proj_closed = grey_closing(backscan_proj.squeeze(), size=(5, 5))\n",
    "\n",
    "axs[1].imshow(backscan_proj_closed, cmap=\"gray\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"Projected backscan morphologically closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spinediffusion.datamodule.sl_generator import SLGenerator\n",
    "\n",
    "sl_generator = SLGenerator(\n",
    "    sl_mean=[\n",
    "        [2.05, 0.52, -58.00],\n",
    "        [2.05, 0.52, -58.00],\n",
    "        [2.05, 0.52, -58.00],\n",
    "        [2.05, 0.52, -58.00],\n",
    "    ],\n",
    "    sl_std=[\n",
    "        [3.4, 0, 4.5],\n",
    "        [14.31, 115.69, 25.92],\n",
    "        [14.31, 115.69, 25.92],\n",
    "        [3.4, 0, 4.5],\n",
    "    ],\n",
    "    length=200,\n",
    "    sample_method=\"normal\",\n",
    "    num_spl_points=1024,\n",
    "    project_args={\n",
    "        \"height\": 128,\n",
    "        \"width\": 128,\n",
    "        \"intensity\": 1,\n",
    "        \"spine_factor\": 1,\n",
    "        \"method\": \"median\",\n",
    "        \"z_lims\": [-150, 100],\n",
    "    },\n",
    ")\n",
    "\n",
    "depthmap, sl, control_points = next(sl_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot the control points\n",
    "control_points = np.array(control_points)\n",
    "ax.scatter(\n",
    "    control_points[:, 0],\n",
    "    control_points[:, 2],\n",
    "    control_points[:, 1],\n",
    "    label=\"Control Points\",\n",
    "    s=100,\n",
    ")\n",
    "\n",
    "# Plot the spline\n",
    "ax.plot(sl[:, 0], sl[:, 2], sl[:, 1], label=\"Spline\", c=\"r\", lw=2)\n",
    "\n",
    "# Set all axes to the same scale\n",
    "lim_max, lim_min = np.max(np.abs(control_points)), -np.max(np.abs(control_points))\n",
    "ax.set_xlim(lim_min, lim_max)\n",
    "ax.set_ylim(lim_min, lim_max)\n",
    "ax.set_zlim(lim_min, lim_max)\n",
    "\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel(\"X (mm)\")\n",
    "ax.set_ylabel(\"Z (mm)\")\n",
    "ax.set_zlabel(\"Y (mm)\")\n",
    "# ax.set_title(\"Control points and fitted spline\", fontsize=20)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(fontsize=20)\n",
    "\n",
    "# Show the plot\n",
    "plt.subplots_adjust(left=-0.1, right=2, top=2, bottom=-0.1)\n",
    "plt.show()\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111)\n",
    "ax2.imshow(depthmap.squeeze(), cmap=\"gray\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random gaussian noise of size 128x128 and display it\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "noise = np.random.normal(0, 1, (128, 128))\n",
    "plt.imshow(noise, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spinediffusion.datamodule.transforms.augmenting import RandomRotationAugmentation\n",
    "\n",
    "augmentor = RandomRotationAugmentation((-1, 1))\n",
    "\n",
    "data_id = {}\n",
    "data_id[\"backscan\"] = backscan\n",
    "data_id[\"special_points\"] = {k: np.array(v) for k, v in metadata[\"specialPts\"].items()}\n",
    "\n",
    "aug_samples = []\n",
    "for i in range(4):\n",
    "    aug_samples.append(augmentor(data_id))\n",
    "\n",
    "# project all of them\n",
    "aug_samples_proj = []\n",
    "\n",
    "for aug_sample in aug_samples:\n",
    "    aug_samples_proj.append(projector(aug_sample))\n",
    "\n",
    "backscan_proj = projector(data_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a grid of the augmented and projected backscans,\n",
    "# the first row is a 3d scatter plot of the backscans and the\n",
    "# the second row is the projected backscans\n",
    "\n",
    "fig = plt.figure(figsize=(30, 12))\n",
    "\n",
    "ax = fig.add_subplot(2, 5, 1, projection=\"3d\")\n",
    "\n",
    "# color the points according to their z coordinate\n",
    "c = np.asarray(backscan.points)[:, 2]\n",
    "ax.scatter(\n",
    "    np.asarray(backscan.points)[:, 0],\n",
    "    np.asarray(backscan.points)[:, 2],\n",
    "    np.asarray(backscan.points)[:, 1],\n",
    "    c=c,\n",
    "    cmap=\"viridis\",\n",
    "    s=0.1,\n",
    ")\n",
    "ax.set_title(\"Original\")\n",
    "\n",
    "lim_max, lim_min = np.max(np.abs(np.asarray(backscan.points))), -np.max(\n",
    "    np.abs(np.asarray(backscan.points))\n",
    ")\n",
    "\n",
    "ax.set_xlim(lim_min, lim_max)\n",
    "ax.set_ylim(lim_min, lim_max)\n",
    "ax.set_zlim(lim_min, lim_max)\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel(\"X (mm)\")\n",
    "ax.set_ylabel(\"Z (mm)\")\n",
    "ax.set_zlabel(\"Y (mm)\")\n",
    "\n",
    "\n",
    "for i, (aug_sample, aug_sample_proj) in enumerate(zip(aug_samples, aug_samples_proj)):\n",
    "\n",
    "    ax = fig.add_subplot(2, 5, i + 2, projection=\"3d\")\n",
    "\n",
    "    # color the points according to their z coordinate\n",
    "    c = np.asarray(aug_sample[\"backscan\"].points)[:, 2]\n",
    "    ax.scatter(\n",
    "        np.asarray(aug_sample[\"backscan\"].points)[:, 0],\n",
    "        np.asarray(aug_sample[\"backscan\"].points)[:, 2],\n",
    "        np.asarray(aug_sample[\"backscan\"].points)[:, 1],\n",
    "        c=c,\n",
    "        cmap=\"viridis\",\n",
    "        s=0.1,\n",
    "    )\n",
    "\n",
    "    if i == 2:\n",
    "        ax.annotate(\n",
    "            \"Augmented\",\n",
    "            xy=(-0.15, 1.05),\n",
    "            xycoords=\"axes fraction\",\n",
    "            ha=\"center\",\n",
    "        )\n",
    "\n",
    "    # set lims to max coordinate of the backscan in any axis\n",
    "\n",
    "    lim_max, lim_min = np.max(\n",
    "        np.abs(np.asarray(aug_sample[\"backscan\"].points))\n",
    "    ), -np.max(np.abs(np.asarray(aug_sample[\"backscan\"].points)))\n",
    "    ax.set_xlim(lim_min, lim_max)\n",
    "    ax.set_ylim(lim_min, lim_max)\n",
    "    ax.set_zlim(lim_min, lim_max)\n",
    "\n",
    "    # switch axis direction from negative to positive to positive to negative\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # remove grid, add axis labels in mm, add a title to each row with dataset\n",
    "    ax.set_xlabel(\"X (mm)\")\n",
    "    ax.set_ylabel(\"Z (mm)\")\n",
    "    ax.set_zlabel(\"Y (mm)\")\n",
    "\n",
    "ax = fig.add_subplot(2, 5, 6)\n",
    "ax.imshow(backscan_proj[\"depth_map\"].squeeze(), cmap=\"gray\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "for i, aug_sample_proj in enumerate(aug_samples_proj):\n",
    "    ax = fig.add_subplot(2, 5, i + 7)\n",
    "    ax.imshow(aug_sample_proj[\"depth_map\"].squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotable_backscans = [\n",
    "    backscan,\n",
    "    aug_samples[0][\"backscan\"],\n",
    "    aug_samples[1][\"backscan\"],\n",
    "    aug_samples[2][\"backscan\"],\n",
    "    aug_samples[3][\"backscan\"],\n",
    "]\n",
    "\n",
    "for plotable_backscan in plotable_backscans:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # color the points according to their z coordinate\n",
    "    c = np.asarray(plotable_backscan.points)[:, 2]\n",
    "    ax.scatter(\n",
    "        np.asarray(plotable_backscan.points)[:, 0],\n",
    "        np.asarray(plotable_backscan.points)[:, 2],\n",
    "        np.asarray(plotable_backscan.points)[:, 1],\n",
    "        c=c,\n",
    "        cmap=\"viridis\",\n",
    "        s=0.1,\n",
    "    )\n",
    "\n",
    "    # set lims to max coordinate of the backscan in any axis\n",
    "\n",
    "    lim_max, lim_min = np.max(np.abs(np.asarray(plotable_backscan.points))), -np.max(\n",
    "        np.abs(np.asarray(plotable_backscan.points))\n",
    "    )\n",
    "    ax.set_xlim(lim_min, lim_max)\n",
    "    ax.set_ylim(lim_min, lim_max)\n",
    "    ax.set_zlim(lim_min, lim_max)\n",
    "\n",
    "    # switch axis direction from negative to positive to positive to negative\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # remove grid, add axis labels in mm, add a title to each row with dataset\n",
    "    ax.set_xlabel(\"X (mm)\")\n",
    "    ax.set_ylabel(\"Z (mm)\")\n",
    "    ax.set_zlabel(\"Y (mm)\")\n",
    "\n",
    "    plt.subplots_adjust(left=-0.1, right=2, top=2, bottom=-0.1)\n",
    "    fig.savefig(\n",
    "        f\"C:\\\\Users\\\\paual\\\\OneDrive - ETH Zurich\\\\semester_project\\\\report\\\\figures\\\\backscan_{i}.pdf\",\n",
    "        dpi=300,\n",
    "    )\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotable_backscans_proj = [\n",
    "    backscan_proj[\"depth_map\"],\n",
    "    aug_samples_proj[0][\"depth_map\"],\n",
    "    aug_samples_proj[1][\"depth_map\"],\n",
    "    aug_samples_proj[2][\"depth_map\"],\n",
    "    aug_samples_proj[3][\"depth_map\"],\n",
    "]\n",
    "\n",
    "for plotable_backscan_proj in plotable_backscans_proj:\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(plotable_backscan_proj.squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    fig.savefig(\n",
    "        f\"C:\\\\Users\\\\paual\\\\OneDrive - ETH Zurich\\\\semester_project\\\\report\\\\figures\\\\backscan_proj_{i}.pdf\",\n",
    "        dpi=300,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "import torch\n",
    "\n",
    "depthmap = torch.tensor(backscan_proj[\"depth_map\"].copy())\n",
    "\n",
    "# tile depthmap tensor 10 times at the first dimension\n",
    "depthmap = depthmap.unsqueeze(0).repeat(11, 1, 1)\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "noise = torch.randn(depthmap.shape) * 0.1\n",
    "timesteps = torch.LongTensor([0, 100, 200, 300, 400, 500, 600, 700, 800, 900, 999])\n",
    "\n",
    "noisy_images = noise_scheduler.add_noise(depthmap, noise, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a row of images from the noisy_images tensor\n",
    "# add a title below each image with the timesteps from 0 to 900 in steps of 100\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(11):\n",
    "    ax = fig.add_subplot(1, 11, i + 1)\n",
    "    ax.imshow(noisy_images[i].squeeze(), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "    # the title should be below the image\n",
    "    ax.set_title(f\"T = {timesteps[i]}\", y=-0.3)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dirs_df = dirs_df[dirs_df[\"processed\"] == True]\n",
    "\n",
    "metadata = {}\n",
    "backscans = {}\n",
    "\n",
    "for backscan_dir, metadata_dir in tqdm(\n",
    "    zip(processed_dirs_df[\"backscan\"], processed_dirs_df[\"metadata\"]),\n",
    "    total=len(processed_dirs_df),\n",
    "):\n",
    "    with open(metadata_dir, \"r\") as f:\n",
    "        metadata_ = json.load(f)\n",
    "\n",
    "    unique_id = f\"{metadata_['dataset']}_{metadata_['id']}\"\n",
    "\n",
    "    metadata[unique_id] = metadata_\n",
    "    backscans[unique_id] = o3d.io.read_point_cloud(backscan_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backscans_dataset = {\n",
    "    \"balgrist\": [],\n",
    "    \"croatian\": [],\n",
    "    \"italian\": [],\n",
    "    \"ukbb\": [],\n",
    "}\n",
    "\n",
    "esl_dataset = {\n",
    "    \"balgrist\": [],\n",
    "    \"croatian\": [],\n",
    "    \"italian\": [],\n",
    "    \"ukbb\": [],\n",
    "}\n",
    "\n",
    "isl_dataset = {\n",
    "    \"balgrist\": [],\n",
    "    \"croatian\": [],\n",
    "    \"italian\": [],\n",
    "    \"ukbb\": [],\n",
    "}\n",
    "\n",
    "for unique_id in metadata:\n",
    "    dataset = metadata[unique_id][\"dataset\"]\n",
    "    esl = metadata[unique_id][\"esl\"]\n",
    "    isl = metadata[unique_id][\"isl\"]\n",
    "\n",
    "    backscans_dataset[dataset].append(backscans[unique_id])\n",
    "    esl_dataset[dataset].append(esl)\n",
    "    isl_dataset[dataset].append(isl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize point distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in backscans_dataset:\n",
    "    points = np.concatenate(\n",
    "        [np.asarray(pc.points) for pc in backscans_dataset[dataset]]\n",
    "    )\n",
    "    x, y, z = points.T\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "    axs[0].hist(x, bins=100, color=\"r\", alpha=0.7)\n",
    "    axs[0].set_title(f\"{dataset} x-axis\")\n",
    "    axs[0].set_xlabel(\"x-axis\")\n",
    "    axs[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    axs[1].hist(y, bins=100, color=\"g\", alpha=0.7)\n",
    "    axs[1].set_title(f\"{dataset} y-axis\")\n",
    "    axs[1].set_xlabel(\"y-axis\")\n",
    "    axs[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    axs[2].hist(z, bins=100, color=\"b\", alpha=0.7)\n",
    "    axs[2].set_title(f\"{dataset} z-axis\")\n",
    "    axs[2].set_xlabel(\"z-axis\")\n",
    "    axs[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in esl_dataset:\n",
    "    key = \"pcdicomapp\"\n",
    "    if dataset == \"croatian\":\n",
    "        key = \"formetric\"\n",
    "    points = np.concatenate([np.asarray(pc[key]) for pc in esl_dataset[dataset]])\n",
    "    x, y, z = points.T\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "    axs[0].hist(x, bins=100, color=\"r\", alpha=0.7)\n",
    "    axs[0].set_title(f\"{dataset} x-axis\")\n",
    "    axs[0].set_xlabel(\"x-axis\")\n",
    "    axs[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    axs[1].hist(y, bins=100, color=\"g\", alpha=0.7)\n",
    "    axs[1].set_title(f\"{dataset} y-axis\")\n",
    "    axs[1].set_xlabel(\"y-axis\")\n",
    "    axs[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    axs[2].hist(z, bins=100, color=\"b\", alpha=0.7)\n",
    "    axs[2].set_title(f\"{dataset} z-axis\")\n",
    "    axs[2].set_xlabel(\"z-axis\")\n",
    "    axs[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in esl_dataset:\n",
    "    key = \"pcdicomapp\"\n",
    "    if dataset == \"croatian\":\n",
    "        key = \"formetric\"\n",
    "    points = np.concatenate([np.asarray(pc[key]) for pc in esl_dataset[dataset]])\n",
    "    x, y, z = points.T\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "    axs[0].hist(x, bins=100, color=\"r\", alpha=0.7)\n",
    "    axs[0].set_title(f\"{dataset} x-axis\")\n",
    "    axs[0].set_xlabel(\"x-axis\")\n",
    "    axs[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "    axs[1].hist(y, bins=100, color=\"g\", alpha=0.7)\n",
    "    axs[1].set_title(f\"{dataset} y-axis\")\n",
    "    axs[1].set_xlabel(\"y-axis\")\n",
    "    axs[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "    axs[2].hist(z, bins=100, color=\"b\", alpha=0.7)\n",
    "    axs[2].set_title(f\"{dataset} z-axis\")\n",
    "    axs[2].set_xlabel(\"z-axis\")\n",
    "    axs[2].set_ylabel(\"Frequency\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unique_id in metadata:\n",
    "    fix_points = np.asarray(metadata[unique_id][\"fix_points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"balgrist_10\"][\"specialPts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"balgrist_1\"][\"specialPts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get common pipelineSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_steps = []\n",
    "order = []\n",
    "\n",
    "for unique_id in metadata:\n",
    "    pipeline_steps.append(metadata[unique_id][\"pipelineSteps\"])\n",
    "    order.append(metadata[unique_id][\"pipelineSteps\"][\"order\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(order[0]).intersection(*order[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline steps applied to all samples (above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_steps[0][\"preprocessing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.asarray(pc.points)\n",
    "\n",
    "points = points / points.max(axis=0)\n",
    "\n",
    "norm_pc = o3d.geometry.PointCloud(points=o3d.utility.Vector3dVector(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([norm_pc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
